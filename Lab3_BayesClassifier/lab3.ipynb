{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "23122040 Nguyễn Văn Linh\n",
        "\n",
        "23122022 Trần Hoàng Gia Bảo\n",
        "\n",
        "23122026 Trần Chấn Hiệp\n",
        "\n",
        "23122040 Nguyễn Thị Mỹ Kim\n",
        "\n",
        "Bài làm chạy trong colab notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nJ3CJU5PoIy",
        "outputId": "f9cce4a5-530a-4939-e9bc-baee8106a9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.4.26)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy pandas numpy matplotlib seaborn nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "x3LhWdwZQJDT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from typing import Dict, List, Tuple\n",
        "import csv\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2C7ELiZSefI",
        "outputId": "18c28450-7238-46a1-8a35-66036dcaaf61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "B9XJeRxaSOtZ"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Khởi tạo lemmatizer\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        \"\"\"\n",
        "        text: một đoạn text (cụ thể ở đây là text kết hợp của subject và messasge)\n",
        "        Làm sạch text - loại bỏ non-words\n",
        "        \"\"\"\n",
        "        if pd.isna(text) or text == '':\n",
        "            return ''\n",
        "\n",
        "        # Chuyển về chữ thường\n",
        "        text = text.lower()\n",
        "\n",
        "        # Loại bỏ email addresses\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "\n",
        "        # Loại bỏ URLs\n",
        "        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
        "\n",
        "        # Loại bỏ HTML tags\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "        # Loại bỏ số (numbers)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Loại bỏ dấu câu và ký tự đặc biệt, chỉ giữ lại chữ cái\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
        "\n",
        "        # Loại bỏ ký tự tabs và xuống dòng\n",
        "        text = re.sub(r'[\\t\\n\\r\\f\\v]', ' ', text)\n",
        "\n",
        "        # Loại bỏ khoảng trắng thừa\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    def tokenize_and_lemmatize(self, text):\n",
        "        \"\"\"\n",
        "        text: một đoạn text (cụ thể ở đây là text kết hợp của subject và messasge)\n",
        "        Tách từ, loại bỏ stop words và thực hiện lemmatization đưa từ về định dạng gốc\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return []\n",
        "\n",
        "        words = word_tokenize(text)\n",
        "\n",
        "        processed_words = []\n",
        "        for word in words:\n",
        "            if (word not in self.stop_words and # nếu từ không phải stop_word\n",
        "                len(word) >= 3 and  # lấy các word có độ dài lớn hơn 3 (từ quá ngắn có thể là noise)\n",
        "                len(word) <= 20): # lấy các word có độ dài nhỏ hơn 15 (từ quá dài có thể là noise))\n",
        "\n",
        "                # đưa từ về dạng gốc\n",
        "                lemmatized_word = self.lemmatizer.lemmatize(word)\n",
        "                processed_words.append(lemmatized_word)\n",
        "\n",
        "        return processed_words\n",
        "\n",
        "    def preprocess_email(self, subject, message, max_tokens=50):\n",
        "        \"\"\"\n",
        "        tiền xử lý 1 email (1 hàng của dữ liệu): làm sạch (sử dụng hàm clean_text và hàm tokenize_and_lemmatize)\n",
        "        subject: tiêu đề thư\n",
        "        message: nội dung thư\n",
        "        max_tokens: giới hạn số token để tránh quá nhiều noise\n",
        "        \"\"\"\n",
        "        # kết hợp subject và message\n",
        "        combined_text = str(subject) + ' ' + str(message)\n",
        "\n",
        "        # làm sạch văn bản\n",
        "        cleaned_text = self.clean_text(combined_text)\n",
        "\n",
        "        # Tokenize và lemmatize\n",
        "        tokens = self.tokenize_and_lemmatize(cleaned_text)\n",
        "\n",
        "        # kiểm tra số token\n",
        "        if len(tokens) > max_tokens:\n",
        "            tokens = tokens[:max_tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def preprocess_data(self, df, remove_duplicate=True):\n",
        "        \"\"\"\n",
        "        Tiền xử lý 1 dataset(train/val)\n",
        "        df: dataframe, đảm bảo tồn tại 3 cột subject, message, spam/ham (label)\n",
        "        \"\"\"\n",
        "        # Thay thế giá trị NaN bằng chuỗi rỗng\n",
        "        df['Subject'] = df['Subject'].fillna('')\n",
        "        df['Message'] = df['Message'].fillna('')\n",
        "\n",
        "        if remove_duplicate:\n",
        "          # Kiểm tra dữ liệu trùng lặp\n",
        "          duplicates = df.duplicated(subset=['Subject', 'Message'])\n",
        "          print(f\"Số lượng email trùng lặp: {duplicates.sum()}\")\n",
        "\n",
        "          # Loại bỏ trùng lặp nếu có\n",
        "          df = df.drop_duplicates(subset=['Subject', 'Message']).reset_index(drop=True)\n",
        "          print(f\"Số lượng email sau khi loại bỏ trùng lặp: {len(df)}\")\n",
        "\n",
        "        # Tiền xử lý từng email\n",
        "        processed_emails = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            tokens = self.preprocess_email(row['Subject'], row['Message'], max_tokens=50)\n",
        "\n",
        "            processed_emails.append({\n",
        "                'tokens': tokens,\n",
        "                'label': row['Spam/Ham'],\n",
        "                'original_subject': row['Subject'],\n",
        "                'original_message': row['Message']\n",
        "            })\n",
        "        return processed_emails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJXgU-m4bGgn"
      },
      "source": [
        "## Giải thích các công thức và thành phần trong mô hình Naive Bayes\n",
        "\n",
        "### 1. Xác suất tiên nghiệm của lớp (Prior Probability)\n",
        "- Đây là xác suất xuất hiện của mỗi nhãn/lớp trước khi nhìn vào dữ liệu đầu vào.\n",
        "- Công thức:  \n",
        "  **P(class) = Số lượng tài liệu thuộc lớp đó / Tổng số tài liệu**\n",
        "\n",
        "### 2. Xác suất có điều kiện của từ cho trước lớp (Likelihood)\n",
        "- Với từng từ trong văn bản, tính xác suất nó xuất hiện trong một lớp cụ thể.\n",
        "- Có 2 cách ước lượng:\n",
        "  - **MLE (Maximum Likelihood Estimation):**  \n",
        "    P(word | class) = Số lần từ xuất hiện trong lớp / Tổng số từ trong lớp\n",
        "  - **MAP (Maximum A Posteriori Estimation):**  \n",
        "    P(word | class) = (count + α) / (total + α * vocab_size)  \n",
        "    (α là tham số Laplace smoothing, thường = 1)\n",
        "\n",
        "### 3. Dự đoán nhãn bằng xác suất hậu nghiệm (Posterior Probability)\n",
        "- Tính xác suất hậu nghiệm của từng lớp với một văn bản mới, sau đó chọn lớp có xác suất cao nhất.\n",
        "- Dùng công thức Bayes:  \n",
        "  **P(class | tokens) ∝ P(class) × Π P(word | class)**\n",
        "- Vì tích của nhiều số nhỏ dễ gây tràn số, ta dùng log:\n",
        "  **log P(class | tokens) = log P(class) + Σ log P(word | class)**\n",
        "\n",
        "### 4. Xử lý từ không thấy trong tập huấn luyện\n",
        "- Với MLE, xác suất có thể bằng 0 nếu từ chưa từng xuất hiện ⇒ log(0) bị lỗi.\n",
        "- Giải pháp: dùng smoothing nhỏ như log(1e-10).\n",
        "- Với MAP, Laplace smoothing giúp đảm bảo mọi từ đều có xác suất > 0.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wloFldHogPt2"
      },
      "outputs": [],
      "source": [
        "class NaiveBayesClassifier:\n",
        "    \"\"\"\n",
        "    Naive Bayes Classifier với MLE và MAP estimation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha=1.0, estimation_method='MAP', min_word_freq=2):\n",
        "        \"\"\"\n",
        "        alpha: tham số Laplace smoothing cho MAP\n",
        "        method: 'MLE' hoặc 'MAP'\n",
        "        min_word_freq: Tần số cần xuất hiện tối thiểu của 1 token trong 1 email để từ đó được giữ lại\n",
        "        \"\"\"\n",
        "        self.alpha = alpha\n",
        "        self.estimation_method = estimation_method\n",
        "        self.min_word_freq = min_word_freq\n",
        "\n",
        "        # vocabulary là các unique tokens trong dataset\n",
        "        self.vocabulary = set()\n",
        "        # đếm số lượng emails theo từng class, ví dụ class_counts[\"spam\"] = 101, nghĩa là có 101 emails spam trong dataset\n",
        "        self.class_counts = {}\n",
        "        # đếm frequency của từng từ trong class \"spam\" và \"ham\", ví dụ, word_counts[\"spam\"][\"hello\"] = 51, nghĩa là có 51 email có từ hello đc đánh dấu là spam\n",
        "        self.word_counts = {}\n",
        "        # tính class priors P(class), ví dụ P[\"spam\"] = số lượng email spam / tổng email\n",
        "        self.class_priors = {}\n",
        "        # là P(word|class), ví dụ P[\"spam\"][\"hello\"] = 0.2\n",
        "        self.word_probs = {}\n",
        "        # số lượng emails\n",
        "        self.total_docs = 0\n",
        "        # số lượng unique tokens trong dataset\n",
        "        self.vocab_size = 0\n",
        "\n",
        "    def mle_estimation(self):\n",
        "        \"\"\"Maximum Likelihood Estimation\"\"\"\n",
        "\n",
        "        self.word_probs = defaultdict(dict)\n",
        "\n",
        "        for class_name in self.class_counts:\n",
        "            total_words_in_class = sum(self.word_counts[class_name].values())\n",
        "\n",
        "            for word in self.vocabulary:\n",
        "                word_count = self.word_counts[class_name].get(word, 0)\n",
        "                # MLE: P(word|class) = count(word, class) / count(class)\n",
        "                self.word_probs[class_name][word] = word_count / total_words_in_class if total_words_in_class > 0 else 0\n",
        "\n",
        "    def map_estimation(self):\n",
        "        \"\"\"Maximum A Posteriori Estimation với Laplace smoothing\"\"\"\n",
        "\n",
        "        self.word_probs = defaultdict(dict)\n",
        "\n",
        "        for class_name in self.class_counts:\n",
        "            total_words_in_class = sum(self.word_counts[class_name].values())\n",
        "\n",
        "            for word in self.vocabulary:\n",
        "                word_count = self.word_counts[class_name].get(word, 0)\n",
        "                # MAP với Laplace smoothing: P(word|class) = (count + alpha) / (total + alpha * vocab_size)\n",
        "                self.word_probs[class_name][word] = (word_count + self.alpha) / (total_words_in_class + self.alpha * len(self.vocabulary))\n",
        "\n",
        "    def fit(self, emails):\n",
        "        \"\"\"Huấn luyện mô hình với MLE hoặc MAP\"\"\"\n",
        "\n",
        "        # Reset parameters\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.word_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        # vocabulary là unique tokens trong dataset\n",
        "        self.vocabulary = set()\n",
        "\n",
        "        # Đếm số lượng emails theo từng class và xây dựng vocabulary\n",
        "        for email in emails:\n",
        "            tokens = email['tokens']\n",
        "            label = email['label']\n",
        "\n",
        "            self.class_counts[label] += 1\n",
        "            self.vocabulary.update(tokens)\n",
        "\n",
        "            # Đếm frequency của từng từ trong class\n",
        "            for token in tokens:\n",
        "                self.word_counts[label][token] += 1\n",
        "\n",
        "        self.total_docs = len(emails)\n",
        "        self.vocab_size = len(self.vocabulary)\n",
        "\n",
        "        # Tính class priors P(class)\n",
        "        for class_name in self.class_counts:\n",
        "            self.class_priors[class_name] = self.class_counts[class_name] / self.total_docs\n",
        "\n",
        "        # Parameter estimation\n",
        "        if self.estimation_method == 'MLE':\n",
        "            self.mle_estimation()\n",
        "        else:  # MAP\n",
        "            self.map_estimation()\n",
        "\n",
        "        # Thống kê\n",
        "        print(f\"Số lượng vocabulary: {self.vocab_size}\")\n",
        "        print(f\"Class distribution: {dict(self.class_counts)}\")\n",
        "        print(f\"Class priors: {dict(self.class_priors)}\")\n",
        "\n",
        "    def predict_proba(self, tokens):\n",
        "        \"\"\"Tính log probabilities cho từng class\"\"\"\n",
        "        scores = {}\n",
        "\n",
        "        # Chỉ xét các từ có trong selected features\n",
        "        relevant_tokens = [token for token in tokens if token in self.vocabulary]\n",
        "\n",
        "        for class_name in self.class_counts:\n",
        "            # Bắt đầu với log prior: log P(class)\n",
        "            log_prob = math.log(self.class_priors[class_name])\n",
        "\n",
        "            # Tính log likelihood: sum(log P(word|class))\n",
        "            for token in relevant_tokens:\n",
        "                if token in self.word_probs[class_name]:\n",
        "                    word_prob = self.word_probs[class_name][token]\n",
        "\n",
        "                    # Tránh log(0)\n",
        "                    if word_prob > 0:\n",
        "                        log_prob += math.log(word_prob)\n",
        "                    else:\n",
        "                        # Xử lý từ không xuất hiện (chỉ với MLE)\n",
        "                        if self.estimation_method == 'MLE':\n",
        "                            log_prob += math.log(1e-10)  # Smoothing nhỏ\n",
        "\n",
        "            scores[class_name] = log_prob\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def predict_one(self, tokens):\n",
        "        \"\"\"predict cho một email\"\"\"\n",
        "        scores = self.predict_proba(tokens)\n",
        "        return max(scores, key=scores.get)\n",
        "\n",
        "    def predict_many(self, emails_tokens):\n",
        "        \"\"\"predict cho nhiều emails\"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        for i, tokens in enumerate(emails_tokens):\n",
        "            pred = self.predict_one(tokens)\n",
        "            predictions.append(pred)\n",
        "\n",
        "        return predictions\n",
        "\n",
        "def eval(model, eval_emails):\n",
        "    test_tokens = [email['tokens'] for email in eval_emails]\n",
        "    true_labels = [email['label'] for email in eval_emails]\n",
        "\n",
        "    # predict trên toàn bộ data eval\n",
        "    predictions = model.predict_many(test_tokens)\n",
        "\n",
        "    # tính accuracy\n",
        "    correct = sum(1 for true, pred in zip(true_labels, predictions) if true == pred)\n",
        "    accuracy = correct / len(true_labels)\n",
        "\n",
        "    # tính F1 cho từng class\n",
        "    metrics = {}\n",
        "    f1_scores = []\n",
        "\n",
        "    for class_name in sorted(set(true_labels)):\n",
        "        tp = sum(1 for true, pred in zip(true_labels, predictions) if true == pred == class_name)\n",
        "        fp = sum(1 for true, pred in zip(true_labels, predictions) if true != class_name and pred == class_name)\n",
        "        fn = sum(1 for true, pred in zip(true_labels, predictions) if true == class_name and pred != class_name)\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "        metrics[class_name] = {'precision': precision, 'recall': recall, 'f1': f1}\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "    # f1 tổng\n",
        "    f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Số lượng dự đoán đúng: {correct}/{len(true_labels)}\")\n",
        "    print(f\"F1-score (Average): {f1:.4f}\")\n",
        "\n",
        "    for class_name, m in metrics.items():\n",
        "        print(f\"{class_name}: Precision: {m['precision']:.4f}, Recall: {m['recall']:.4f}, F1-score: {m['f1']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qJoiL0flRUQ"
      },
      "source": [
        "## TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BBTgzUISkUPC",
        "outputId": "4959b82d-c444-4a7c-89ca-5ceb395d864f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 27284,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9724,\n        \"min\": 0,\n        \"max\": 33715,\n        \"num_unique_values\": 27284,\n        \"samples\": [\n          5123,\n          12402,\n          12979\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9724,\n        \"min\": 0,\n        \"max\": 33715,\n        \"num_unique_values\": 27284,\n        \"samples\": [\n          5123,\n          12402,\n          12979\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20231,\n        \"samples\": [\n          \"investment idea\",\n          \"paracodin sells better than vicodin\",\n          \"for the last three decades , brokerage houses - - such as merryll lynch - - have cashed in with greater returns than any other industry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Message\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24451,\n        \"samples\": [\n          \"hows it been going ?\\nyou have been chosen to participate in an invitation\\nonly event !\\nare you currently paying over 3 % for your mortgage ?\\nstop ! we can help you lower that today !\\nanswer only a few questions and we can get you\\napproved in under 1 minute , it ' s that simple !\\nmore info here : anyhgh . com\\n$ 302 , 000 loans are available for only $ 231 / month !\\neveryone is approved !\\nbad credit ?\\nno problem ! we ' ll have you saving money in no time !\\nare you ready to save ?\\njust fill out our short form : anyhgh . com\\nthanks alot ,\\nbaez v . jodie , v\\nprojecthoneypot @ projecthoneypot . org\\nthe secret of life is honesty and fair dealing . if you can fake that , you ' ve got it made . - groucho marx ( 1890 - 1977 ) . from that day on there were new rules in my classroom . each student was to have an adult ' communication partner ' at the computer . this adult was to sit with the child , not saying a word until the student stopped and looked at the adult or in some other way indicated that communication was desired . then the adult was only to encourage the student by saying the word , nodding the headand smiling . the student was allowed to continue his or her learning . when the student imitated a word , the adult was to respond appropriately . no questions were allowed during this beginning phase . the students were just learning to talk . .\\nluke is missing jumping today . . few things are harder to put up with than the annoyance of a good example . .\\nall my life i ' ve wanted to be someone ; i guess i should have been more specific . jane wagner / lily tomlin ( 1939 - ) . i am not missing surfing . .\",\n          \"discount drugs . . . save 80 % every order !\\nwe are the number one online retailler for dozens of medications . our customers save 80\\ncents out of every dollar , every time , compared to the industry price . yes , that is less than\\nquarter - price\\nwe have all the products that our customers have asked for , including new superviagra\\nsoft - tabs that work in just 15 minutes ! this is the next - generation of sexual improvement\\nwonder - drugs , far more effective than viagra - half a pill will last for 36 hours !\\nget all the information on superviagra here : http : / / friggings . net / cs / ? cheapgeneric\\nour key to keeping customers satisfied is :\\neasy ordering online\\nsave 80 % on regular price\\nwe have massive stocks of drugs for same day dispatch\\nfast delivery straight to your door with discrete packaging\\nwe are the biggest internet retailler with thousands of regular customers\\nno consultation fee\\nno intimate questions or examinations\\nno appointment\\nno prior prescription needed\\nprivate and confidential service\\nplease come by our shop , see for yourself the massive range of products that we have\\navailable . we do have the lowest price and huge stocks ready for same - day dispatch .\\ntwo million customers can ' t be wrong !\\nsee our full range at http : / / friggings . net / ? cheapgeneric\\n\",\n          \"per my voicemails to each of you . . . .\\njenny latham is currently a manager in power risk in houston , and is\\ninterested in moving to london . she is very well respected within enron , and\\nis a highly rated employee . she has been with enron for several years , in\\nvarious positions , and will move us far ahead in the staffing . in january , i\\nmet with yvonne scorer to identify recruiting needs to build the mid and back\\noffice for the london activity for eim . one position identified , business\\ncontroller , is the position i have discussed with jenny , and she is very\\ninterested . i am anxious to move this forward asap for a couple of reasons :\\n1 ) . jenny is interested , and i do not want to lose her to other\\nopportunities within the london office , and 2 ) . it is not too early to move\\nforward on the staffing of the office as it will take some time to\\nappropriately staff and transition current support from egm .\\nbruce , last week jenny and i met with you , and we discussed that it would be\\nappropriate for jenny to spend time with us here in houston to learn our\\ncurrent processes , commoditiies , etc . i absolutely agree with this , but we\\nneed to make a deal with her before we can start that process . the sequence\\nof events that make sense to me and that i believe would make jenny the most\\ncomfortable is that we 1 ) . agree on the financial package ( compensation ,\\nmoving allowance , etc ) , 2 ) . transition her current responsibilities within\\npower risk , and 3 ) . begin training with us . simultaneously , we will need to\\nrecruit applicants for the remaining positions in risk , documentation ,\\nsettlements and trade accounting . my goal is to have everything in place in\\nlondon by the end of first quarter .\\nto meet this time line , we need to get more information and have a discussion\\nwith meryl on the specifics . bruce , can you give meryl the green light to\\nmove forward with us ? i appreciate your help .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Spam/Ham\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"split\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0024066869151718,\n        \"min\": -3.842532500480177,\n        \"max\": 3.6127381957087294,\n        \"num_unique_values\": 27284,\n        \"samples\": [\n          0.9783164143218958,\n          -1.0624398833040851\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-feaa11be-676f-4ae8-bdd1-8aeaf2d765fc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Message ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>0.038415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>0.696509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>0.587792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>-0.055438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>mcmullen gas for 11 / 99</td>\n",
              "      <td>jackie ,\\nsince the inlet to 3 river plant is ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>-0.419658</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-feaa11be-676f-4ae8-bdd1-8aeaf2d765fc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-feaa11be-676f-4ae8-bdd1-8aeaf2d765fc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-feaa11be-676f-4ae8-bdd1-8aeaf2d765fc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-9423a828-6f2f-4f72-a11d-02bf9a5d10f2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9423a828-6f2f-4f72-a11d-02bf9a5d10f2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-9423a828-6f2f-4f72-a11d-02bf9a5d10f2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   Unnamed: 0  Message ID                       Subject  \\\n",
              "0           0           0  christmas tree farm pictures   \n",
              "1           1           1      vastar resources , inc .   \n",
              "2           2           2  calpine daily gas nomination   \n",
              "3           3           3                    re : issue   \n",
              "4           5           5      mcmullen gas for 11 / 99   \n",
              "\n",
              "                                             Message Spam/Ham     split  \n",
              "0                                                NaN      ham  0.038415  \n",
              "1  gary , production from the high island larger ...      ham  0.696509  \n",
              "2             - calpine daily gas nomination 1 . doc      ham  0.587792  \n",
              "3  fyi - see note below - already done .\\nstella\\...      ham -0.055438  \n",
              "4  jackie ,\\nsince the inlet to 3 river plant is ...      ham -0.419658  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# đọc dữ liệu training\n",
        "df_train = pd.read_csv('/content/sample_data/train.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAkNr9AjkLfX"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "# Tiền xử lí dữ liệu cho dữ liệu train\n",
        "preprocessor = TextPreprocessor()\n",
        "df_train = preprocessor.preprocess_data(df_train, remove_duplicate=True)\n",
        "\n",
        "# MLE\n",
        "print(\"Training với mô hình thống kê MLE\")\n",
        "model_mle = NaiveBayesClassifier(\n",
        "    estimation_method='MLE',\n",
        "    alpha=1.0,\n",
        "    min_word_freq=3\n",
        ")\n",
        "model_mle.fit(df_train)\n",
        "\n",
        "# MAP\n",
        "print(\"Training với mô hình thống kê MAP\")\n",
        "model_map = NaiveBayesClassifier(\n",
        "    estimation_method='MAP',\n",
        "    min_word_freq=3\n",
        ")\n",
        "model_map.fit(df_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLm_7GUClafh"
      },
      "source": [
        "## CODE BLOCK CHỨC NĂNG 1: Người dùng nhập vào một email bất kì (gồm tiêu đề và nội dung)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOx56OZenWE2",
        "outputId": "fbff27cd-8455-4f07-a749-51263a0be11d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nhập thông tin email bạn muốn kiểm tra.\n",
            "Tiêu đề (Subject): how are you?\n",
            "Nội dung (Message): hi, what's your name?\n",
            "Kết quả dự đoán từ mô hình MLE: SPAM\n",
            "Kết quả dự đoán từ mô hình MAP: SPAM\n"
          ]
        }
      ],
      "source": [
        "def predict_custom_email(subject, message, model, preprocessor):\n",
        "    preprocessor = TextPreprocessor()\n",
        "    processed_tokens = preprocessor.preprocess_email(subject, message)\n",
        "    prediction = model.predict_one(processed_tokens)\n",
        "    return prediction\n",
        "\n",
        "# yêu cầu nhập thông tin\n",
        "print(\"Nhập thông tin email bạn muốn kiểm tra.\")\n",
        "user_subject = input(\"Tiêu đề (Subject): \")\n",
        "user_message = input(\"Nội dung (Message): \")\n",
        "\n",
        "if user_subject.strip() or user_message.strip():\n",
        "    # Kết quả của mô hình MLE\n",
        "    prediction_mle = predict_custom_email(user_subject, user_message, model_mle, preprocessor)\n",
        "    print(f\"Kết quả dự đoán từ mô hình MLE: {prediction_mle.upper()}\")\n",
        "\n",
        "    # Kết quả của mô hình MAP\n",
        "    prediction_map = predict_custom_email(user_subject, user_message, model_map, preprocessor)\n",
        "    print(f\"Kết quả dự đoán từ mô hình MAP: {prediction_map.upper()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2FwzaQTlGPp"
      },
      "source": [
        "## CODE BLOCK CHỨC NĂNG 2: Eval trên tập dữ liệu tương tự \"val.csv\"\n",
        "\n",
        "Thầy thay đường dẫn val_path đến file csv trong code block sau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_6NzzcHgSgb",
        "outputId": "cd9f0dde-6dd6-4387-e28a-ce2d2b0a3243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval trên mô hình MAP\n",
            "Accuracy: 0.9812\n",
            "Số lượng dự đoán đúng: 3026/3084\n",
            "F1-score (Average): 0.9812\n",
            "ham: Precision: 0.9747, Recall: 0.9875, F1-score: 0.9811\n",
            "spam: Precision: 0.9877, Recall: 0.9750, F1-score: 0.9813\n",
            "Eval trên mô hình MLE\n",
            "Accuracy: 0.9822\n",
            "Số lượng dự đoán đúng: 3029/3084\n",
            "F1-score (Average): 0.9822\n",
            "ham: Precision: 0.9785, Recall: 0.9855, F1-score: 0.9820\n",
            "spam: Precision: 0.9858, Recall: 0.9789, F1-score: 0.9823\n"
          ]
        }
      ],
      "source": [
        "# evaluate\n",
        "# đường dẫn tới file csv cần eval\n",
        "val_path = '/content/sample_data/val.csv'\n",
        "df_val = pd.read_csv(val_path)\n",
        "# tiền xử lí dữ liệu (do eval nên không loại bỏ dữ liệu trùng lặp)\n",
        "preprocessor = TextPreprocessor()\n",
        "df_val = preprocessor.preprocess_data(df_val, remove_duplicate=False)\n",
        "# eval trên mô hình MAP\n",
        "print(\"Eval trên mô hình MAP\")\n",
        "eval(model_map, df_val)\n",
        "# eval trên mô hình MLE\n",
        "print(\"Eval trên mô hình MLE\")\n",
        "eval(model_mle, df_val)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
